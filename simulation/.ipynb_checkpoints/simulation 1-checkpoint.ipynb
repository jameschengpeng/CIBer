{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pengcheng/opt/anaconda3/lib/python3.7/site-packages/lightgbm/__init__.py:48: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.insert(1, '/Users/pengcheng/Desktop/CIBer')\n",
    "import comonotonic as cm\n",
    "import ensemble_ciber as ec\n",
    "import random\n",
    "from scipy.stats import gamma\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import utils\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "n_basic_var = 10 # num of basic variables\n",
    "max_var = 10 # maximum variance of basic variable\n",
    "n_class = 3 # num of classes\n",
    "max_slope = 10 # maximum slope of X' = k*X + b\n",
    "variance = 100 # variance of intersection i.e. X' = k*X + b, Var(b) = variance\n",
    "var_noise = 10 # variance of the noise term\n",
    "n_como_var = 5 # num of como variables generated by each basic variable\n",
    "instance_per_class = 10000 # how large is the simulated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct n_class centers representing the classes\n",
    "centers = list()\n",
    "for i in range(n_class):\n",
    "    center = list()\n",
    "    for j in range(n_basic_var):\n",
    "        rv = gamma.rvs(np.random.uniform(0,100))\n",
    "        center.append(rv)\n",
    "    centers.append(center)\n",
    "centers = np.array(centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct simulated data for basic variables\n",
    "simulated_data = list()\n",
    "for i in range(n_class):\n",
    "    class_col = np.array([i for itr in range(instance_per_class)]).reshape(-1,1)\n",
    "    cov = np.zeros((n_basic_var, n_basic_var), dtype = float)\n",
    "    for j in range(n_basic_var):\n",
    "        cov[j][j] = np.random.uniform(max_var)\n",
    "    class_data = np.random.multivariate_normal(centers[i], cov, instance_per_class)\n",
    "    class_data = np.concatenate((class_col, class_data), axis = 1)\n",
    "    simulated_data.append(class_data)\n",
    "simulated_data = np.array(simulated_data).reshape(n_class*instance_per_class, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct simulated data for comonotonic variables\n",
    "for i in range(n_basic_var):\n",
    "    for j in range(n_como_var):\n",
    "        slope = np.random.uniform(max_slope)\n",
    "        intersection = np.random.normal(0, variance)\n",
    "        noise = np.random.normal(0,var_noise,simulated_data.shape[0])\n",
    "        como_var = (simulated_data[:,i+1]*slope + intersection + noise).reshape(-1,1)\n",
    "        simulated_data = np.concatenate((simulated_data, como_var), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = simulated_data[:,1:]\n",
    "Y = simulated_data[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_col = [i for i in range(X.shape[1])]\n",
    "categorical = []\n",
    "discrete_feature_val = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00      2010\n",
      "         1.0       1.00      1.00      1.00      1961\n",
      "         2.0       1.00      1.00      1.00      2029\n",
      "\n",
      "    accuracy                           1.00      6000\n",
      "   macro avg       1.00      1.00      1.00      6000\n",
      "weighted avg       1.00      1.00      1.00      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ciber = cm.clustered_comonotonic(X_train,Y_train,discrete_feature_val,cont_col,categorical,\n",
    "                                0.9,None,corrtype = 'pearson',discrete_method = \"mdlp\")\n",
    "ciber.run()\n",
    "ciber_predict = ciber.predict(X_test)\n",
    "print(classification_report(Y_test, ciber_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 3, 10, 11, 12, 13, 14, 25, 26, 28, 29], [1, 8, 15, 16, 17, 18, 19, 50, 51, 52, 53, 54], [2, 21, 22, 24], [4, 6, 30, 31, 32, 33, 34, 41, 42, 43, 44], [5, 7, 35, 36, 37, 38, 39, 45, 46, 47, 48, 49], [9, 55, 56, 57, 59], [20], [23], [27], [40], [58]]\n"
     ]
    }
   ],
   "source": [
    "ciber.print_cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00      2010\n",
      "         1.0       1.00      1.00      1.00      1961\n",
      "         2.0       1.00      1.00      1.00      2029\n",
      "\n",
      "    accuracy                           1.00      6000\n",
      "   macro avg       1.00      1.00      1.00      6000\n",
      "weighted avg       1.00      1.00      1.00      6000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ciber_nb = cm.clustered_comonotonic(X_train,Y_train,discrete_feature_val,\n",
    "                                    cont_col,categorical,1,None,\n",
    "                                    corrtype = 'mutual_info',discrete_method = \"mdlp\")\n",
    "ciber_nb.run()\n",
    "ciber_nb_predict = ciber_nb.predict(X_test)\n",
    "print(classification_report(Y_test, ciber_nb_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       194\n",
      "         1.0       1.00      1.00      1.00       206\n",
      "\n",
      "    accuracy                           1.00       400\n",
      "   macro avg       1.00      1.00      1.00       400\n",
      "weighted avg       1.00      1.00      1.00       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_clf = xgb.XGBClassifier()\n",
    "xgb_clf.fit(X_train, Y_train)\n",
    "xgb_predict = xgb_clf.predict(X_test)\n",
    "print(classification_report(Y_test,xgb_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       194\n",
      "         1.0       1.00      1.00      1.00       206\n",
      "\n",
      "    accuracy                           1.00       400\n",
      "   macro avg       1.00      1.00      1.00       400\n",
      "weighted avg       1.00      1.00      1.00       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_clf = RandomForestClassifier()\n",
    "rf_clf.fit(X_train, Y_train)\n",
    "rf_predict = rf_clf.predict(X_test)\n",
    "print(classification_report(Y_test,rf_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       194\n",
      "         1.0       1.00      1.00      1.00       206\n",
      "\n",
      "    accuracy                           1.00       400\n",
      "   macro avg       1.00      1.00      1.00       400\n",
      "weighted avg       1.00      1.00      1.00       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ada_clf = AdaBoostClassifier()\n",
    "ada_clf.fit(X_train, Y_train)\n",
    "ada_predict = ada_clf.predict(X_test)\n",
    "print(classification_report(Y_test,ada_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       194\n",
      "         1.0       1.00      1.00      1.00       206\n",
      "\n",
      "    accuracy                           1.00       400\n",
      "   macro avg       1.00      1.00      1.00       400\n",
      "weighted avg       1.00      1.00      1.00       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lgb_clf = lgb.LGBMClassifier()\n",
    "lgb_clf.fit(X_train, Y_train)\n",
    "lgb_predict = lgb_clf.predict(X_test)\n",
    "print(classification_report(Y_test,lgb_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       194\n",
      "         1.0       1.00      1.00      1.00       206\n",
      "\n",
      "    accuracy                           1.00       400\n",
      "   macro avg       1.00      1.00      1.00       400\n",
      "weighted avg       1.00      1.00      1.00       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt_clf = DecisionTreeClassifier()\n",
    "dt_clf.fit(X_train, Y_train)\n",
    "dt_predict = dt_clf.predict(X_test)\n",
    "print(classification_report(Y_test, dt_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       194\n",
      "         1.0       1.00      1.00      1.00       206\n",
      "\n",
      "    accuracy                           1.00       400\n",
      "   macro avg       1.00      1.00      1.00       400\n",
      "weighted avg       1.00      1.00      1.00       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_clf = SVC()\n",
    "svm_clf.fit(X_train, Y_train)\n",
    "svm_predict = svm_clf.predict(X_test)\n",
    "print(classification_report(Y_test, svm_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       194\n",
      "         1.0       1.00      1.00      1.00       206\n",
      "\n",
      "    accuracy                           1.00       400\n",
      "   macro avg       1.00      1.00      1.00       400\n",
      "weighted avg       1.00      1.00      1.00       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(X_train, Y_train)\n",
    "lr_predict = lr_clf.predict(X_test)\n",
    "print(classification_report(Y_test, lr_predict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
